import io
import os
import zipfile
from pathlib import Path
from typing import List, Optional
from uuid import UUID

from fastapi import APIRouter, Depends, File, HTTPException, Query, UploadFile, WebSocket, status
from sqlalchemy.ext.asyncio import AsyncSession
import httpx
from pydantic import BaseModel

from app.core.config import get_settings
from app.db import get_db_session
from app.models import AnalysisStatus
from app.schemas.analysis import AnalysisTaskCreateResponse, AnalysisTaskListItem, AnalysisTaskResponse
from app.services import analysis_tasks as analysis_tasks_service
from app.services.files_service import FilesService
from app.services.queue import rabbitmq_client
from app.services.websocket_manager import task_ws_manager

router = APIRouter()

settings = get_settings()
files_service = FilesService()

SUPPORTED_EXTENSIONS = {
    ".jpg",
    ".jpeg",
    ".png",
    ".tif",
    ".tiff",
    ".bmp",
    ".dng",
    ".raw",
    ".nef",
    ".cr2",
    ".arw",
}

RAW_EXTENSIONS = {".dng", ".raw", ".nef", ".cr2", ".arw"}


async def _ensure_size(upload_file) -> int:
    file_obj = upload_file.file
    file_obj.seek(0, os.SEEK_END)
    size = file_obj.tell()
    file_obj.seek(0)
    return size


@router.post(
    "/predict/batch",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=AnalysisTaskCreateResponse,
)
async def create_batch_task(
    files: List[UploadFile] = File(..., description="–°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"),
    conf: float = Query(0.35, ge=0.0, le=1.0, description="–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"),
    route_name: Optional[str] = Query(None, max_length=250, description="–ù–∞–∑–≤–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞"),
    preview_limit: int = Query(
        default=settings.PREVIEW_LIMIT,
        ge=1,
        le=settings.PREVIEW_LIMIT,
        description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–≤—å—é –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏",
    ),
    session: AsyncSession = Depends(get_db_session),
):
    if not files:
        raise HTTPException(status_code=400, detail="–ù–µ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

    if len(files) > settings.MAX_BATCH_FILES:
        raise HTTPException(
            status_code=400,
            detail=f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –≤ –±–∞—Ç—á–µ: {settings.MAX_BATCH_FILES}",
        )

    total_bytes = 0
    file_descriptors = []
    for upload in files:
        extension = Path(upload.filename or "").suffix.lower()
        if extension in {".zip", ".tar"}:
            raise HTTPException(
                status_code=400,
                detail="ZIP/TAR –∞—Ä—Ö–∏–≤—ã –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è. –ó–∞–≥—Ä—É–∂–∞–π—Ç–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.",
            )

        if extension not in SUPPORTED_EXTENSIONS:
            raise HTTPException(
                status_code=400,
                detail=f"–§–æ—Ä–º–∞—Ç {extension or 'unknown'} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. "
                       f"–†–∞–∑—Ä–µ—à–µ–Ω—ã: {', '.join(sorted(SUPPORTED_EXTENSIONS))}",
            )

        size = await _ensure_size(upload)
        total_bytes += size
        file_descriptors.append((upload, extension, size))

    if total_bytes > settings.MAX_BATCH_SIZE_BYTES:
        raise HTTPException(
            status_code=400,
            detail=f"–°—É–º–º–∞—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤ –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å 10 –ì–ë",
        )

    task = await analysis_tasks_service.create_task(
        session,
        total_files=len(file_descriptors),
        total_bytes=total_bytes,
        confidence_threshold=conf,
        preview_limit=min(preview_limit, settings.PREVIEW_LIMIT),
        route_name=route_name,
    )

    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N —Ñ–∞–π–ª–æ–≤ –∫–∞–∫ –ø—Ä–µ–≤—å—é, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–¥–∞—ë–º –≤ worker
    preview_limit_upload = settings.UPLOAD_PREVIEW_LIMIT
    stored_images = []
    archive_zip = io.BytesIO()

    # –†–∞–∑–¥–µ–ª—è–µ–º —Ñ–∞–π–ª—ã –Ω–∞ preview –∏ –∞—Ä—Ö–∏–≤–Ω—ã–µ
    preview_files = []
    archive_files = []

    for idx, (upload, extension, size) in enumerate(file_descriptors):
        upload.file.seek(0)
        file_content = await upload.read()
        upload.file.seek(0)

        if idx < preview_limit_upload:
            preview_files.append(upload)
        else:
            archive_files.append((upload, extension, file_content, idx))

    # Batch-–∑–∞–≥—Ä—É–∑–∫–∞ preview —Ñ–∞–π–ª–æ–≤ (–æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –≤–º–µ—Å—Ç–æ N)
    if preview_files:
        try:
            batch_result = await files_service.batch_upload_files(
                files=preview_files,
                project_id=str(task.id),
                file_type="ANALYSIS_ORIGINAL",
                uploaded_by=None,
            )

            for uploaded_file in batch_result.get("files", []):
                stored_images.append(
                    {
                        "file_id": UUID(uploaded_file["id"]),
                        "file_name": uploaded_file["file_name"],
                        "file_size": uploaded_file["file_size"],
                    }
                )
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to batch upload preview files: {str(e)}"
            )

    # –ê—Ä—Ö–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–æ–±–∞–≤–ª—è–µ–º –≤ ZIP
    with zipfile.ZipFile(archive_zip, "w", compression=zipfile.ZIP_DEFLATED) as zip_file:
        for upload, extension, file_content, idx in archive_files:
            safe_name = Path(upload.filename or f"file_{idx}{extension}").name
            zip_file.writestr(safe_name, file_content)
            await upload.close()

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º preview —Ñ–∞–π–ª—ã
    for upload in preview_files:
        await upload.close()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞—Ä—Ö–∏–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ worker (–±—É–¥–µ—Ç —É–¥–∞–ª—ë–Ω –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏)
    archive_zip.seek(0)
    archive_data = archive_zip.getvalue() if archive_zip.getvalue() else None

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞—Ä—Ö–∏–≤ –≤ files-service (worker —Å–∫–∞—á–∞–µ—Ç, –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –∏ —É–¥–∞–ª–∏—Ç)
    if archive_data and len(archive_data) > 0:
        archive_upload = await files_service.upload_bytes(
            data=archive_data,
            filename=f"{task.id}_temp_uploaded_archive.zip",
            content_type="application/zip",
            project_id=str(task.id),
            file_type="ANALYSIS_ARCHIVE",  # –í—Ä–µ–º–µ–Ω–Ω—ã–π, –±—É–¥–µ—Ç —É–¥–∞–ª—ë–Ω worker'–æ–º
        )
        await analysis_tasks_service.set_task_archives(
            session,
            task.id,
            originals_archive_id=UUID(archive_upload["id"]),
        )

    await analysis_tasks_service.add_images(
        session,
        task_id=task.id,
        images=stored_images,
    )

    await rabbitmq_client.publish_task(
        {
            "task_id": str(task.id),
            "confidence_threshold": conf,
            "preview_limit": preview_limit,
        }
    )

    return AnalysisTaskCreateResponse(task_id=task.id, status=task.status)


@router.get("/analysis/tasks/{task_id}", response_model=AnalysisTaskResponse)
async def get_task(
    task_id: UUID,
    session: AsyncSession = Depends(get_db_session),
):
    task = await analysis_tasks_service.get_task(session, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    previews = [
        image
        for image in task.images
        if image.is_preview
    ]

    return AnalysisTaskResponse(
        id=task.id,
        status=task.status,
        route_name=task.route_name,
        total_files=task.total_files,
        total_bytes=task.total_bytes,
        processed_files=task.processed_files,
        failed_files=task.failed_files,
        defects_found=task.defects_found,
        confidence_threshold=task.confidence_threshold,
        preview_limit=task.preview_limit,
        message=task.message,
        originals_archive_file_id=task.originals_archive_file_id,
        results_archive_file_id=task.results_archive_file_id,
        created_at=task.created_at,
        updated_at=task.updated_at,
        completed_at=task.completed_at,
        metadata=task.task_metadata,
        preview_files=[
            {
                "id": image.id,
                "file_id": image.file_id,
                "file_name": image.file_name,
                "file_size": image.file_size,
                "status": image.status,
                "is_preview": image.is_preview,
                "summary": image.summary,
                "result_file_id": image.result_file_id,
                "error_message": image.error_message,
            }
            for image in previews
        ],
    )


@router.get("/analysis/history", response_model=List[AnalysisTaskListItem])
async def list_tasks(
    limit: int = Query(20, ge=1, le=100),
    session: AsyncSession = Depends(get_db_session),
):
    tasks = await analysis_tasks_service.list_tasks(session, limit=limit)
    return [
        AnalysisTaskListItem(
            id=task.id,
            status=task.status,
            route_name=task.route_name,
            total_files=task.total_files,
            processed_files=task.processed_files,
            defects_found=task.defects_found,
            created_at=task.created_at,
            completed_at=task.completed_at,
        )
        for task in tasks
    ]


@router.get("/analysis/tasks/{task_id}/images")
async def get_task_images(
    task_id: UUID,
    skip: int = Query(0, ge=0, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"),
    limit: int = Query(100, ge=1, le=500, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"),
    include_thumbnails: bool = Query(False, description="–í–∫–ª—é—á–∏—Ç—å thumbnails –≤ base64 (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)"),
    session: AsyncSession = Depends(get_db_session),
):
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ URL –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
    task = await analysis_tasks_service.get_task(session, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    images, total = await analysis_tasks_service.get_task_images(session, task_id, skip=skip, limit=limit)

    # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ summary —Å detections
    import logging
    logger = logging.getLogger(__name__)
    for img in images:
        if img.summary and img.summary.get("detections"):
            detections = img.summary.get("detections", [])
            manual_count = len([d for d in detections if d.get("is_manual")])
            if manual_count > 0:
                logger.info(f"üì§ Returning image {img.id} with {len(detections)} detections ({manual_count} manual)")
                logger.info(f"üìã Manual detections details: {[d.get('class_ru', d.get('class', 'unknown')) for d in detections if d.get('is_manual')]}")

    # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω—ã thumbnails, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏—Ö batch'–æ–º
    thumbnails_data = {}
    if include_thumbnails and images:
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ file_id –¥–ª—è batch download (—Ç–æ–ª—å–∫–æ result —Ñ–∞–π–ª—ã –¥–ª—è –ø—Ä–µ–≤—å—é)
            file_ids_to_fetch = []
            for image in images:
                # –î–ª—è thumbnail –∏—Å–ø–æ–ª—å–∑—É–µ–º result (—Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏), –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ original
                file_id = str(image.result_file_id if image.result_file_id else image.file_id)
                file_ids_to_fetch.append(file_id)

            # Batch download –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
            if file_ids_to_fetch:
                batch_result = await files_service.batch_download_files(file_ids_to_fetch)

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64 –¥–ª—è thumbnails (–º–∞–ª–µ–Ω—å–∫–∏–µ –ø—Ä–µ–≤—å—é)
                from PIL import Image
                import io
                import base64

                for file_data in batch_result.get("files", []):
                    try:
                        # –°–æ–∑–¥–∞–µ–º thumbnail (resize –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ payload)
                        img = Image.open(io.BytesIO(file_data["content"]))

                        # Resize –¥–æ 400px –ø–æ —à–∏—Ä–∏–Ω–µ –¥–ª—è thumbnail
                        max_width = 400
                        if img.width > max_width:
                            ratio = max_width / img.width
                            new_size = (max_width, int(img.height * ratio))
                            img = img.resize(new_size, Image.Resampling.LANCZOS)

                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JPEG –∏ base64
                        buffer = io.BytesIO()
                        img.convert('RGB').save(buffer, format='JPEG', quality=85, optimize=True)
                        thumbnail_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')

                        thumbnails_data[file_data["file_id"]] = f"data:image/jpeg;base64,{thumbnail_base64}"
                    except Exception as e:
                        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å thumbnail, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        pass
        except Exception as e:
            # –ï—Å–ª–∏ batch download –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ—Å—Ç–æ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º thumbnails
            pass

    return {
        "total": total,
        "skip": skip,
        "limit": limit,
        "images": [
            {
                "id": image.id,
                "file_id": image.file_id,
                "file_name": image.file_name,
                "file_size": image.file_size,
                "status": image.status,
                "is_preview": image.is_preview,
                "summary": image.summary,
                "result_file_id": image.result_file_id,
                "error_message": image.error_message,
                "created_at": image.created_at,
                # URL –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                "original_url": f"/api/files/{image.file_id}/view",
                # URL –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                "result_url": f"/api/files/{image.result_file_id}/view" if image.result_file_id else None,
                # Thumbnail –≤ base64 (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)
                "thumbnail": thumbnails_data.get(
                    str(image.result_file_id if image.result_file_id else image.file_id)
                ) if include_thumbnails else None,
            }
            for image in images
        ],
    }


@router.delete("/analysis/tasks/{task_id}/images/{image_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task_image(
    task_id: UUID,
    image_id: UUID,
    session: AsyncSession = Depends(get_db_session),
):
    """–£–¥–∞–ª–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –∑–∞–¥–∞—á–∏"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
    task = await analysis_tasks_service.get_task(session, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    # –£–¥–∞–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    deleted_image_data = await analysis_tasks_service.delete_image(session, task_id, image_id)
    if not deleted_image_data:
        raise HTTPException(status_code=404, detail="–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –ö–æ–º–º–∏—Ç–∏–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
    await session.commit()

    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ files-service
    try:
        file_ids_to_delete = deleted_image_data.get('file_ids_to_delete', [])
        for file_id in file_ids_to_delete:
            try:
                await files_service.delete_file(str(file_id))
            except Exception as e:
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É, –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
                print(f"Failed to delete file {file_id}: {e}")
    except Exception as e:
        print(f"Error deleting files: {e}")

    return None


@router.delete("/analysis/tasks/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(
    task_id: UUID,
    session: AsyncSession = Depends(get_db_session),
):
    """–£–¥–∞–ª–∏—Ç—å –∑–∞–¥–∞—á—É –∏ –≤—Å–µ –µ—ë –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
    task = await analysis_tasks_service.get_task(session, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    # –£–¥–∞–ª—è–µ–º –∑–∞–¥–∞—á—É –∏ –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    deleted_task_data = await analysis_tasks_service.delete_task(session, task_id)
    if not deleted_task_data:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    # –ö–æ–º–º–∏—Ç–∏–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
    await session.commit()

    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ files-service
    try:
        file_ids_to_delete = deleted_task_data.get('file_ids_to_delete', [])
        for file_id in file_ids_to_delete:
            try:
                await files_service.delete_file(str(file_id))
            except Exception as e:
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É, –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
                print(f"Failed to delete file {file_id}: {e}")
    except Exception as e:
        print(f"Error deleting files: {e}")

    return None


@router.websocket("/ws/tasks/{task_id}")
async def task_updates(task_id: str, websocket: WebSocket):
    try:
        task_uuid = UUID(task_id)
    except ValueError:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return

    await task_ws_manager.connect(task_uuid, websocket)
    try:
        while True:
            await websocket.receive_text()
    except Exception:
        pass
    finally:
        task_ws_manager.disconnect(task_uuid, websocket)


@router.websocket("/ws/history")
async def history_updates(websocket: WebSocket):
    """
    WebSocket endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤—Å–µ—Ö –∑–∞–¥–∞—á –≤ –∏—Å—Ç–æ—Ä–∏–∏.
    –ö–ª–∏–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ —Å—Ç–∞—Ç—É—Å–µ –ª—é–±–æ–π –∑–∞–¥–∞—á–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.
    """
    await task_ws_manager.connect_history(websocket)
    try:
        while True:
            # –ü—Ä–æ—Å—Ç–æ –¥–µ—Ä–∂–∏–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–º
            # –ö–ª–∏–µ–Ω—Ç –º–æ–∂–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å ping, –Ω–æ –º—ã –∏—Ö –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
            await websocket.receive_text()
    except Exception:
        pass
    finally:
        task_ws_manager.disconnect_history(websocket)


class BBox(BaseModel):
    x: int
    y: int
    width: int
    height: int
    name: Optional[str] = None
    is_defect: Optional[bool] = True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ


class AnnotationRequest(BaseModel):
    file_id: str
    bboxes: List[BBox]
    project_id: str
    file_type: str = "ANALYSIS_RESULT"


class ImageMetric(BaseModel):
    """–ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
    detection_id: Optional[str] = None  # ID –¥–µ—Ç–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –ø—Ä–∏–≤—è–∑–∞–Ω–∞ –∫ –Ω–µ–π
    class_name: str  # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –æ–±—ä–µ–∫—Ç–∞
    class_name_ru: Optional[str] = None  # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    confidence: float  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0-1)
    bbox: List[float]  # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox [x1, y1, x2, y2]
    defect_type: Optional[str] = None  # –¢–∏–ø –¥–µ—Ñ–µ–∫—Ç–∞: "damage", "missing", "normal"
    severity: Optional[str] = None  # –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: "high", "medium", "low", "none"
    description: Optional[str] = None  # –û–ø–∏—Å–∞–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–∞
    is_manual: bool = False  # –†—É—á–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è


class ImageMetricsRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    metrics: List[ImageMetric]  # –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫
    total_objects: Optional[int] = None  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤
    defects_count: Optional[int] = None  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ñ–µ–∫—Ç–æ–≤
    has_defects: Optional[bool] = None  # –ï—Å—Ç—å –ª–∏ –¥–µ—Ñ–µ–∫—Ç—ã
    statistics: Optional[dict] = None  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º


@router.post("/analysis/tasks/{task_id}/images/{image_id}/annotate")
async def annotate_image(
    task_id: UUID,
    image_id: UUID,
    request: AnnotationRequest,
    db: AsyncSession = Depends(get_db_session)
):
    """
    –î–æ–±–∞–≤–∏—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (bbox) –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ annotation-service

    - **task_id**: ID –∑–∞–¥–∞—á–∏
    - **image_id**: ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    - **request**: –î–∞–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (bboxes, project_id, file_type)
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–¥–∞—á–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        task = await analysis_tasks_service.get_task(db, task_id)
        if not task:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Task not found"
            )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∑–∞–¥–∞—á–µ
        image = await analysis_tasks_service.get_image(db, image_id, task_id=task_id)
        if not image:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Image not found or does not belong to this task"
            )

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º file_id –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (result_file_id –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ file_id)
        file_id_to_annotate = str(image.result_file_id) if image.result_file_id else str(image.file_id)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ annotation-service
        annotation_request = {
            "file_id": file_id_to_annotate,
            "bboxes": [bbox.dict() for bbox in request.bboxes],
            "project_id": request.project_id or str(task_id),
            "file_type": request.file_type or "ANALYSIS_RESULT"
        }

        # –ü—Ä–æ–∫—Å–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ annotation-service
        annotation_service_url = settings.ANNOTATION_SERVICE_URL
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{annotation_service_url}/annotations/annotate",
                json=annotation_request
            )

            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("detail", error_detail)
                except:
                    pass
                raise HTTPException(
                    status_code=response.status_code,
                    detail=error_detail
                )

            result = response.json()

            # –û–±–Ω–æ–≤–ª—è–µ–º result_file_id –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤—ã–º file_id –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            # annotation-service –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç "file_id" –≤ –æ—Ç–≤–µ—Ç–µ
            new_file_id = result.get("file_id")
            import logging
            logger = logging.getLogger(__name__)
            logger.info(f"Annotate result: file_id={new_file_id}, image_id={image_id}")

            if new_file_id:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π summary –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                    current_summary = image.summary or {}

                    # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                    existing_detections = current_summary.get("detections", [])

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –≤ summary
                    annotations = []
                    manual_detections = []

                    for bbox in request.bboxes:
                        annotation = {
                            "x": bbox.x,
                            "y": bbox.y,
                            "width": bbox.width,
                            "height": bbox.height,
                        }
                        if bbox.name:
                            annotation["name"] = bbox.name
                        annotation["is_defect"] = bbox.is_defect if bbox.is_defect is not None else True
                        annotations.append(annotation)

                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä—É—á–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏
                        is_defect = bbox.is_defect if bbox.is_defect is not None else True
                        # –§–æ—Ä–º–∞—Ç bbox: [x1, y1, x2, y2] (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
                        x1, y1 = bbox.x, bbox.y
                        x2, y2 = bbox.x + bbox.width, bbox.y + bbox.height
                        bbox_area = bbox.width * bbox.height

                        manual_detection = {
                            "class": bbox.name or "–†—É—á–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è",
                            "class_ru": bbox.name or "–†—É—á–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è",
                            "confidence": 1.0,  # 100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                            "bbox": [int(x1), int(y1), int(x2), int(y2)],
                            "bbox_size": {
                                "width": bbox.width,
                                "height": bbox.height,
                                "area": bbox_area,
                                "is_small": bbox.width < 30 or bbox.height < 30
                            },
                            "defect_summary": {
                                "type": "–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ" if is_defect else "–ù–æ—Ä–º–∞",
                                "severity": "high" if is_defect else "none",
                                "description": "–†—É—á–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è"
                            },
                            "is_manual": True  # –ú–µ—Ç–∫–∞ —á—Ç–æ —ç—Ç–æ —Ä—É—á–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                        }
                        manual_detections.append(manual_detection)

                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å —Ä—É—á–Ω—ã–º–∏
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ä—É—á–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ (—Å is_manual=True) –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
                    # –í–ê–ñ–ù–û: request.bboxes –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –í–°–ï –º–∞—Å–∫–∏ (—Å—Ç–∞—Ä—ã–µ + –Ω–æ–≤—ã–µ), —Ç–∞–∫ –∫–∞–∫ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
                    filtered_detections = [d for d in existing_detections if not d.get("is_manual", False)]
                    all_detections = filtered_detections + manual_detections

                    # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    logger.info(f"üì¶ Request bboxes count: {len(request.bboxes)}")
                    logger.info(f"üîÑ Existing detections: {len(existing_detections)} (manual: {len([d for d in existing_detections if d.get('is_manual')])})")
                    logger.info(f"‚ú® New manual detections: {len(manual_detections)}")
                    logger.info(f"üìä Total detections after merge: {len(all_detections)} (manual: {len(manual_detections)})")

                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏
                    total_objects = len(all_detections)
                    defects_count = sum(1 for d in all_detections if (
                        d.get("defect_summary", {}).get("type") != "–ù–æ—Ä–º–∞" and
                        d.get("defect_summary", {}).get("severity") not in ["none", None]
                    ))

                    # –û–±–Ω–æ–≤–ª—è–µ–º summary - –í–ê–ñ–ù–û: —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    current_summary = {
                        **current_summary,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                        "detections": all_detections,  # –û–±–Ω–æ–≤–ª—è–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ (–≤–∫–ª—é—á–∞—è —Ä—É—á–Ω—ã–µ)
                        "manual_annotations": annotations,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                        "has_manual_annotations": len(annotations) > 0,
                        "total_objects": total_objects,
                        "defects_count": defects_count,
                        "has_defects": defects_count > 0
                    }

                    # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    logger.info(f"üîÑ Updating image {image_id} summary: total_objects={total_objects}, defects_count={defects_count}, manual_detections={len(manual_detections)}")
                    logger.info(f"üìä Summary detections count: {len(all_detections)}, manual: {len(manual_detections)}")
                    logger.info(f"üìù Manual detections: {[d.get('class_ru', d.get('class', 'unknown')) for d in manual_detections]}")
                    logger.info(f"üíæ Saving summary with {len(all_detections)} detections to DB")
                    logger.info(f"üîç Sample detection: {manual_detections[0] if manual_detections else 'None'}")

                    # –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç summary –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è JSON –ø–æ–ª—è
                    import copy
                    summary_to_save = copy.deepcopy(current_summary)

                    await analysis_tasks_service.update_image(
                        db,
                        image_id,
                        result_file_id=UUID(new_file_id),
                        summary=summary_to_save
                    )
                    await db.commit()
                    logger.info(f"‚úÖ Summary saved to DB for image {image_id}")

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å
                    updated_image = await analysis_tasks_service.get_image(db, image_id)
                    if updated_image and updated_image.summary:
                        saved_detections = updated_image.summary.get("detections", [])
                        saved_manual = [d for d in saved_detections if d.get("is_manual")]
                        logger.info(f"‚úÖ Verified saved: {len(saved_detections)} detections, {len(saved_manual)} manual")
                    else:
                        logger.warning(f"‚ö†Ô∏è Image {image_id} summary is None after save")
                except Exception as e:
                    # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É, –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                    logger.error(f"‚ùå Failed to update image result_file_id: {str(e)}", exc_info=True)
            else:
                logger.warning(f"‚ö†Ô∏è No file_id in annotation result for image {image_id}")

            return result

    except httpx.RequestError as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Cannot connect to annotation service: {str(e)}"
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to annotate image: {str(e)}"
        )


@router.post("/analysis/tasks/{task_id}/images/{image_id}/metrics")
async def save_image_metrics(
    task_id: UUID,
    image_id: UUID,
    request: ImageMetricsRequest,
    db: AsyncSession = Depends(get_db_session)
):
    """
    –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    - **task_id**: ID –∑–∞–¥–∞—á–∏
    - **image_id**: ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    - **request**: –î–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫ (metrics, total_objects, defects_count, etc.)
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–¥–∞—á–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        task = await analysis_tasks_service.get_task(db, task_id)
        if not task:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Task not found"
            )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∑–∞–¥–∞—á–µ
        image = await analysis_tasks_service.get_image(db, image_id, task_id=task_id)
        if not image:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Image not found or does not belong to this task"
            )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        detections = []
        for metric in request.metrics:
            detection = {
                "class": metric.class_name,
                "class_ru": metric.class_name_ru or metric.class_name,
                "confidence": metric.confidence,
                "bbox": metric.bbox,
                "bbox_size": {
                    "width": int(metric.bbox[2] - metric.bbox[0]) if len(metric.bbox) >= 4 else 0,
                    "height": int(metric.bbox[3] - metric.bbox[1]) if len(metric.bbox) >= 4 else 0,
                    "area": int((metric.bbox[2] - metric.bbox[0]) * (metric.bbox[3] - metric.bbox[1])) if len(metric.bbox) >= 4 else 0,
                    "is_small": False
                },
                "defect_summary": {
                    "type": "–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ" if metric.defect_type in ["damage", "missing"] else "–ù–æ—Ä–º–∞",
                    "severity": metric.severity or ("high" if metric.defect_type in ["damage", "missing"] else "none"),
                    "description": metric.description or ""
                },
                "is_manual": metric.is_manual
            }
            if metric.detection_id:
                detection["detection_id"] = metric.detection_id
            detections.append(detection)

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π summary –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
        current_summary = image.summary or {}

        # –û–±–Ω–æ–≤–ª—è–µ–º summary —Å –Ω–æ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        updated_summary = {
            **current_summary,
            "detections": detections,
            "total_objects": request.total_objects or len(detections),
            "defects_count": request.defects_count or sum(1 for m in request.metrics if m.defect_type in ["damage", "missing"]),
            "has_defects": request.has_defects if request.has_defects is not None else (request.defects_count or 0) > 0,
            "statistics": request.statistics or current_summary.get("statistics", {})
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π summary
        await analysis_tasks_service.update_image(
            db,
            image_id,
            summary=updated_summary
        )
        await db.commit()

        return {
            "image_id": str(image_id),
            "metrics_count": len(detections),
            "total_objects": updated_summary["total_objects"],
            "defects_count": updated_summary["defects_count"],
            "has_defects": updated_summary["has_defects"]
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save metrics: {str(e)}"
        )


@router.get("/analysis/tasks/{task_id}/images/{image_id}/metrics")
async def get_image_metrics(
    task_id: UUID,
    image_id: UUID,
    db: AsyncSession = Depends(get_db_session)
):
    """
    –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    - **task_id**: ID –∑–∞–¥–∞—á–∏
    - **image_id**: ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–¥–∞—á–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        task = await analysis_tasks_service.get_task(db, task_id)
        if not task:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Task not found"
            )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∑–∞–¥–∞—á–µ
        image = await analysis_tasks_service.get_image(db, image_id, task_id=task_id)
        if not image:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Image not found or does not belong to this task"
            )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ summary
        summary = image.summary or {}
        detections = summary.get("detections", [])

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –º–µ—Ç—Ä–∏–∫
        metrics = []
        for detection in detections:
            defect_summary = detection.get("defect_summary", {})
            defect_type = None
            if defect_summary.get("type") != "–ù–æ—Ä–º–∞":
                defect_type = "damage" if "–ø–æ–≤—Ä–µ–∂" in defect_summary.get("type", "").lower() else "missing"
            else:
                defect_type = "normal"

            metric = ImageMetric(
                detection_id=detection.get("detection_id"),
                class_name=detection.get("class", ""),
                class_name_ru=detection.get("class_ru", detection.get("class", "")),
                confidence=detection.get("confidence", 0.0),
                bbox=detection.get("bbox", []),
                defect_type=defect_type,
                severity=defect_summary.get("severity"),
                description=defect_summary.get("description"),
                is_manual=detection.get("is_manual", False)
            )
            metrics.append(metric)

        return {
            "image_id": str(image_id),
            "metrics": [m.dict() for m in metrics],
            "total_objects": summary.get("total_objects", len(metrics)),
            "defects_count": summary.get("defects_count", 0),
            "has_defects": summary.get("has_defects", False),
            "statistics": summary.get("statistics", {})
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get metrics: {str(e)}"
        )

